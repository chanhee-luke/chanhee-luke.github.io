<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="I am a CS PhD student at The Ohio State University. I am interested in embodied AI, robotics, and large models.">
<meta property="og:type" content="website">
<meta property="og:title">
<meta property="og:url" content="https://chanhee-luke.github.io/index.html">
<meta property="og:site_name">
<meta property="og:description" content="I am a CS PhD student at The Ohio State University. I am interested in embodied AI, robotics, and large models.">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Chan Hee (Luke) Song">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/logo.png">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/android-chrome-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title></title>
    <!-- async scripts -->
    <!-- Google Analytics -->


    <!-- Umami Analytics -->


    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 7.1.1"></head>

<body class="max-width mx-auto px3 ltr">
    
    <div class="content index py4 h-card">
        
          <header id="header">
  <a class="u-url u-uid" styles="font-size: 1rem;" href="/">
  
    <div id="title">
      <h1 class="p-name"></h1>
    </div>
  </a>
  <div id="nav">
    <ul>
      <li class="icon">
        <a href="#" aria-label="Menu"><i class="fa-solid fa-bars fa-2x"></i></a>
      </li>
      <!--
     --><li><a href="/">About</a></li><!--
   --><!--
     --><li><a href="/publications/">Publications</a></li><!--
   -->
    </ul>
  </div>
</header>

        
        <section id="about" class="p-note">
  <br />

  <div class="about-header">
    <a class="about-header-image">
    <img class="default-image" src="https://raw.githubusercontent.com/chanhee-luke/chanhee-luke.github.io/new_look/images/profile_new.jpg">
    <img class="hover-image" src="https://raw.githubusercontent.com/chanhee-luke/chanhee-luke.github.io/new_look/images/doochoo_new.jpg">
    </a>
    
    <div class="description">
    <p class="name">Hi, I'm Chan Hee (Luke) Song.</p> 
    <p>I am a CS PhD student at <a href="https://www.osu.edu" target="_blank" rel="noopener noreferrer">The Ohio State University</a> advised by <a target="_blank" rel="noopener" href="https://ysu1989.github.io/">Yu Su</a>.</p>
    <p>My research focuses on multimodal agents, particularly on planning, perception, and benchmarking.</p>
    <p>During my undergraduate at <a href="https://www.nd.edu" target="_blank" rel="noopener noreferrer">Notre Dame</a>, I was part of the <a href="https://nlp.nd.edu" target="_blank" rel="noopener noreferrer">ND NLP</a>.</p>
    <p>I have interned at <a href="https://research.nvidia.com/labs/lpr/" target="_blank" rel="noopener noreferrer">Nvidia Research</a> and <a href="https://research.adobe.com" target="_blank" rel="noopener noreferrer">Adobe Research</a>.</p>

    <!-- <p>I received BS Cum Laude in Computer Science from <a href="https://www.nd.edu" target="_blank" rel="noopener noreferrer">University of Notre Dame</a>, where I was part of <a href="https://nlp.nd.edu" target="_blank" rel="noopener noreferrer">ND NLP Group</a>.</p> -->
    <!-- <p><i class="fa-solid fa-location-dot"></i> Columbus, OH</p> -->
    
      <p>
        Find me on
        
        
        
          
          
            <a class="icon u-url" target="_blank" rel="noopener me" href="https://x.com/luke_ch_song" aria-label="twitter" title="twitter">
              <i class="fa-brands fa-twitter"></i><!--
        ---></a><!--
      ---><!--
      --->, 
          
        
          
          
            <a class="icon u-url" target="_blank" rel="noopener me" href="https://www.linkedin.com/in/lukesonggg/" aria-label="linkedin" title="linkedin">
              <i class="fa-brands fa-linkedin"></i><!--
        ---></a><!--
      ---><!--
      --->, 
          
        
          
          
            <a class="icon u-url" target="_blank" rel="noopener me" href="https://github.com/chanhee-luke" aria-label="github" title="github">
              <i class="fa-brands fa-github"></i><!--
        ---></a><!--
      ---><!--
      ---> and 
          
        
          
          
            <a class="icon" target="_blank" rel="noopener" href="https://scholar.google.com/citations?user=IdBL738AAAAJ&hl=en" aria-label="graduation-cap" title="graduation-cap">
              <i class="fa-solid fa-graduation-cap"></i>
            </a>
          <!--
      --->.
          
        
      </p>
    
    </div>
  </div>

  <!-- 
    <p>I am a CS PhD student at The Ohio State University. I am interested in embodied AI, robotics, and large models.</p>

   -->
  
  <br />

  
    <span class="h1">What's New</span>
    <div class="overall-timeline">
        
            <div class="timeline">
                <div class="time-block">
                    <div class="time-circle"></div>
                    <p class="time-month">Feb 2025</p>
                    <p class="time-title">
                        
                        

                      
                        
                        
                      
                      <a target="_blank" rel="noopener" href="https://chanh.ee/RoboSpatial">RoboSpatial</a> has been accepted to CVPR 2025!
                    </p>
                </div>
            </div>
        
            <div class="timeline">
                <div class="time-block">
                    <div class="time-circle"></div>
                    <p class="time-month">Feb 2025</p>
                    <p class="time-title">
                        
                        

                      
                        
                        
                      
                      <a target="_blank" rel="noopener" href="https://github.com/THUDM/VisualAgentBench">VisualAgentBench</a> has been accepted to ICLR 2025.
                    </p>
                </div>
            </div>
        
            <div class="timeline">
                <div class="time-block">
                    <div class="time-circle"></div>
                    <p class="time-month">Nov 2024</p>
                    <p class="time-title">
                        
                        

                      
                        
                        
                      
                      Excited to present <a target="_blank" rel="noopener" href="https://chanh.ee/RoboSpatial">RoboSpatial</a>, a work done at Nvidia. We present a large-scale 2D/3D spatial understanding dataset and benchmark tailored for robotics. Stay tuned for the full release!
                    </p>
                </div>
            </div>
        
            <div class="timeline">
                <div class="time-block">
                    <div class="time-circle"></div>
                    <p class="time-month">Jun 2024</p>
                    <p class="time-title">
                        
                        

                      
                        
                        
                      
                      <a target="_blank" rel="noopener" href="https://imageomics.github.io/bioclip/">BioCLIP</a> won the best student paper award at CVPR 2024! Honored to be part of the team.
                    </p>
                </div>
            </div>
        
            <div class="timeline">
                <div class="time-block">
                    <div class="time-circle"></div>
                    <p class="time-month">Feb 2024</p>
                    <p class="time-title">
                        
                        
                          
                          
                              
                          
                      
                          
                          
                              
                          
                      

                      
                        
                        
                      
                        
                        
                      
                      <a target="_blank" rel="noopener" href="https://imageomics.github.io/bioclip/">BioCLIP</a>, a biology vision foundation model <span class="spotlight">(Oral)</span>, and <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.04476">Dual-VCR</a>, a dual-view web-navigation method <span class="spotlight">(Poster)</span> have been accepted to CVPR 2024!
                    </p>
                </div>
            </div>
        
            <div class="timeline">
                <div class="time-block">
                    <div class="time-circle"></div>
                    <p class="time-month">Feb 2024</p>
                    <p class="time-title">
                        
                        

                      
                        
                        
                      
                      I will be interning at <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/lpr/">Nvidia Learning and Perception Research Group</a> this summer. Catch me in Seattle!
                    </p>
                </div>
            </div>
        
            <div class="timeline">
                <div class="time-block">
                    <div class="time-circle"></div>
                    <p class="time-month">Jul 2023</p>
                    <p class="time-title">
                        
                        

                      
                        
                        
                      
                      <a target="_blank" rel="noopener" href="https://osu-nlp-group.github.io/LLM-Planner/">LLM-Planner</a>, a paper on using large language models for vison-and-language navigation accepted to ICCV 2023.
                    </p>
                </div>
            </div>
        
            <div class="timeline">
                <div class="time-block">
                    <div class="time-circle"></div>
                    <p class="time-month">Mar 2023</p>
                    <p class="time-title">
                        
                        

                      
                        
                        
                      
                      Our <a target="_blank" rel="noopener" href="https://embodied-ai.org/papers/2023/10.pdf">SalsaBot</a> work for Amazon Alexa Prize Challenge has been accepted to the Embodied AI Workshop at CVPR 2023!
                    </p>
                </div>
            </div>
        
            <div class="timeline">
                <div class="time-block">
                    <div class="time-circle"></div>
                    <p class="time-month">Mar 2023</p>
                    <p class="time-title">
                        
                        

                      
                        
                        
                      
                      I will be interning at <a target="_blank" rel="noopener" href="https://research.adobe.com">Adobe Research</a> this summer. Catch me in San Jose!</a>
                    </p>
                </div>
            </div>
        
    </div>


<br />

<div id="selected_pub">
  <span class="h1">Selected Publications</span>
  <p class="tips1">See full list in <a href="/publications">Publications</a>.</p>

  <ul class="post-list">
    
      <li class="post-item">
        <p class="paper-name">RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics</p>
        <p class="authors"><span>Chan Hee Song</span>, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, Stan Birchfield</p>
        <p class="links">
          <span class="tag">CVPR 2025</span>
          <span class="spotlight"></span>

          
            <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.16537">Paper</a>
          

          
            <a target="_blank" rel="noopener" href="https://chanh.ee/RoboSpatial">Website</a>
          

          

          

          

          

        </p>
      </li>
    
      <li class="post-item">
        <p class="paper-name">VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents</p>
        <p class="authors">Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, Jiadai Sun, Xinyue Yang, Yu Yang, Zehan Qi, Shuntian Yao, Xueqiao Sun, Siyi Cheng, Qinkai Zheng, Hao Yu, Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang Pan, Xiaotao Gu, Aohan Zeng, Zhengxiao Du, <span>Chan Hee Song</span>, Yu Su, Yuxiao Dong, Jie Tang</p>
        <p class="links">
          <span class="tag">ICLR 2025</span>
          <span class="spotlight"></span>

          
            <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2408.06327">Paper</a>
          

          

          
            <a target="_blank" rel="noopener" href="https://github.com/THUDM/VisualAgentBench/tree/main">Code</a>
          

          

          

          

        </p>
      </li>
    
      <li class="post-item">
        <p class="paper-name">BioCLIP: A Vision Foundation Model for the Tree of Life</p>
        <p class="authors">Samuel Stevens, Jiaman Wu, Matthew J Thompson, Elizabeth G Campolongo, <span>Chan Hee Song</span>, David Edward Carlyn, Li Dong, Wasila M Dahdul, Charles Stewart, Tanya Berger-Wolf, Wei-Lun Chao, Yu Su</p>
        <p class="links">
          <span class="tag">CVPR 2024</span>
          <span class="spotlight">Best Student Paper Award</span>

          
            <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2311.18803">Paper</a>
          

          
            <a target="_blank" rel="noopener" href="https://imageomics.github.io/bioclip/">Website</a>
          

          
            <a target="_blank" rel="noopener" href="https://github.com/Imageomics/BioCLIP">Code</a>
          

          
            <a target="_blank" rel="noopener" href="https://huggingface.co/datasets/imageomics/TreeOfLife-10M">Data</a>
          

          

          

        </p>
      </li>
    
      <li class="post-item">
        <p class="paper-name">Dual-View Visual Contextualization for Web Navigation</p>
        <p class="authors">Jihyung Kil, <span>Chan Hee Song</span>, Boyuan Zheng, Xiang Deng, Yu Su, Wei-Lun Chao</p>
        <p class="links">
          <span class="tag">CVPR 2024</span>
          <span class="spotlight"></span>

          
            <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.04476">Paper</a>
          

          

          

          

          

          

        </p>
      </li>
    
      <li class="post-item">
        <p class="paper-name">LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models</p>
        <p class="authors"><span>Chan Hee Song</span>, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, Yu Su</p>
        <p class="links">
          <span class="tag">ICCV 2023</span>
          <span class="spotlight"></span>

          
            <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.04088">Paper</a>
          

          
            <a target="_blank" rel="noopener" href="https://osu-nlp-group.github.io/LLM-Planner/">Website</a>
          

          

          

          

          

        </p>
      </li>
    
      <li class="post-item">
        <p class="paper-name">One Step at a Time: Long-Horizon Vision-and-Language Navigation with Milestones</p>
        <p class="authors"><span>Chan Hee Song</span>, Jihyung Kil, Tai-Yu Pan, Brian M. Sadler, Wei-Lun Chao, Yu Su</p>
        <p class="links">
          <span class="tag">CVPR 2022</span>
          <span class="spotlight"></span>

          
            <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2202.07028">Paper</a>
          

          

          

          

          

          

        </p>
      </li>
    
  </ul>
</div>
</section>


<span class="h1">Contact</span>
<div class="contact-content">
<div>Email: <strong>1ch[LAST_NAME]@gmail.com</strong></div>
<p>Feel free to contact me if you are interested in my research or want to discuss anything :)</p>
</div>

<br />
<br />
<br />


        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2024-2025
    Chan Hee (Luke) Song
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">About</a></li><!--
     --><!--
       --><li><a href="/publications/">Publications</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script>




<!-- clipboard -->


<script src="/js/main.js"></script>

<!-- search -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

</body>
</html>
